{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter selection, Validation & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have parameters that influence how complex a model they can learn. Remember using KNeighborsRegressor.\n",
    "If we change the number of neighbors we consider, we get a smoother and smoother prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from figures import plot_kneighbors_regularization\n",
    "plot_kneighbors_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we trade of remembering too much about the particularities and noise of the training data vs not modeling enough of the variability. This is a trade-off that needs to be made in basically every machine learning application and is a centeral concept, called bias-variance-tradeoff or \"overfitting vs underfitting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![underfitting and overfitting](figures/overfitting_underfitting_cartoon.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters, Over-fitting, and Under-fitting\n",
    "Unfortunately, there is no general rule how to find the sweet spot, and so machine learning practitioners have to find the best trade-off of model-complexity and generalization by trying several parameter settings.\n",
    "Most commonly this is done using a brute force search, for example over multiple values of ``n_neighbors``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# generate toy dataset:\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.sin(4 * x) + x + np.random.normal(size=len(x))\n",
    "X = x[:, np.newaxis]\n",
    "\n",
    "cv = KFold(n=len(x), shuffle=True)\n",
    "\n",
    "# for each parameter setting do cross_validation:\n",
    "for n_neighbors in [1, 3, 5, 10, 20]:\n",
    "    scores = cross_val_score(KNeighborsRegressor(n_neighbors=n_neighbors), X, y, cv=cv)\n",
    "    print(\"n_neighbors: %d, average score: %f\" % (n_neighbors, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If multiple parameters are important, like the parameters ``C`` and ``gamma`` in an ``SVM`` (more about that later), all possible combinations are tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# each parameter setting do cross_validation:\n",
    "for C in [0.001, 0.01, 0.1, 1, 10]:\n",
    "    for gamma in [0.001, 0.01, 0.1, 1]:\n",
    "        scores = cross_val_score(SVR(C=C, gamma=gamma), X, y, cv=cv)\n",
    "        print(\"C: %f, gamma: %f, average score: %f\" % (C, gamma, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is such a very common pattern, there is a built-in class for this in scikit-learn, ``GridSearchCV``. ``GridSearchCV`` takes a dictionary that describes the parameters that should be trieda nd a model to train.\n",
    "\n",
    "The grid of paramters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "grid = GridSearchCV(SVR(), param_grid=param_grid, cv=cv, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about GridSearchCV is that it is a *meta-estimator*. It takes an estimator like SVR above, and creates a new estimator, that behaves exactly the same - in this case, like a regressor.\n",
    "So we can call ``fit`` on it, to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ``fit`` does is a bit more involved then what we did above. First, it runs the same loop with cross-validation, to find the best parameter combination.\n",
    "Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to built a single new model using the best parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as with all models, we can use ``predict`` or ``score``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
