{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The content in this section is adapted from Andrew Ng's excellent\n",
    "Coursera course, available here:* https://www.coursera.org/course/ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues associated with validation and \n",
    "cross-validation are some of the most important\n",
    "aspects of the practice of machine learning.  Selecting the optimal model\n",
    "for your data is vital, and is a piece of the problem that is not often\n",
    "appreciated by machine learning practitioners.\n",
    "\n",
    "Of core importance is the following question:\n",
    "\n",
    "**If our estimator is underperforming, how should we move forward?**\n",
    "\n",
    "- Use simpler or more complicated model?\n",
    "- Add more features to each observed data point?\n",
    "- Add more training samples?\n",
    "\n",
    "The answer is often counter-intuitive.  In particular, **sometimes using a\n",
    "more complicated model will give _worse_ results.**  Also, **sometimes adding\n",
    "training data will not improve your results.**  The ability to determine\n",
    "what steps will improve your model is what separates the successful machine\n",
    "learning practitioners from the unsuccessful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves and Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to address this issue is to use what are often called **Learning Curves**.\n",
    "Given a particular dataset and a model we'd like to fit (e.g. using k neighbors regression), we'd\n",
    "like to tune our value of the *hyperparameter* ``n_neighbors`` to give us the best fit.\n",
    "\n",
    "Lets go back to our regression problem from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from figures import plot_kneighbors_regularization\n",
    "plot_kneighbors_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves\n",
    "============\n",
    "\n",
    "What the right model for a dataset is depends critically on how much data we have. More data allows us to be more confident about building a complex model. Lets built some intuition on why that is. Look at the folling datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from figures import plot_regression_datasets\n",
    "plot_regression_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all come from the same underlying process. But if you were asked to make a prediction, you would be more likely to draw a straight line for the left-most one, as there are only very few datapoints, and no real rule is aparent. For the dataset in the middle, some structure is recognizable, though the exact shape of the true function is maybe not obvious. With even more data on the right hand side, you would probably be very comfortable with drawing a sinusoidal line with a lot of certainty.\n",
    "\n",
    "A great way to explore how a model fit evolves with different dataset sizes are learning curves.\n",
    "A learning curve plots the validation error for a given model against different training set sizes.\n",
    "\n",
    "But first, take a moment to think about what we're going to see:\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- **As the number of training samples are increased, what do you expect to see for the training error?  For the validation error?**\n",
    "- **Would you expect the training error to be higher or lower than the validation error?  Would you ever expect this to change?**\n",
    "\n",
    "We can run the following code to plot the learning curve for a ``d=1`` model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXME WORST EXAMPLE EVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from figures import make_dataset\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "x, y = make_dataset(n_samples=100)\n",
    "X = x[:, np.newaxis]\n",
    "\n",
    "training_sizes, train_scores, test_scores = learning_curve(KNeighborsRegressor(n_neighbors=5), X, y, cv=10)\n",
    "plt.plot(training_sizes, train_scores.mean(axis=1), label=\"training scores\")\n",
    "plt.plot(training_sizes, test_scores.mean(axis=1), label=\"test scores\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for the very complex model with ``n_neighbors=2``, the score increase \n",
    "\n",
    "Notice that the validation error *generally decreases* with a growing training set,\n",
    "while the training error *generally increases* with a growing training set.  From\n",
    "this we can infer that as the training size increases, they will converge to a single\n",
    "value.\n",
    "\n",
    "From the above discussion, we know that `d = 1` is a high-bias estimator which\n",
    "under-fits the data. This is indicated by the fact that both the\n",
    "training and validation errors are very high. When confronted with this type of learning curve,\n",
    "we can expect that adding more training data will not help matters: both\n",
    "lines will converge to a relatively high error.\n",
    "\n",
    "**When the learning curves have converged to a high error, we have an underfitting model.**\n",
    "\n",
    "An underfitting model can be improved by:\n",
    "\n",
    "- Using a more sophisticated model (i.e. in this case, increase ``d``)\n",
    "- Gather more features for each sample.\n",
    "- Decrease regularlization in a regularized model.\n",
    "\n",
    "A underfitting model cannot be improved, however, by increasing the number of training\n",
    "samples (do you see why?)\n",
    "\n",
    "Now let's look at an over-fit model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXME ADD EXAMPLE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the learning curve for `d = 20`. From the above\n",
    "discussion, we know that `d = 20` is an estimator\n",
    "which **over-fits** the data. This is indicated by the fact that the\n",
    "training error is much less than the validation error. As\n",
    "we add more samples to this training set, the training error will\n",
    "continue to climb, while the cross-validation error will continue\n",
    "to decrease, until they meet in the middle. In this case, our\n",
    "intrinsic error was set to 1.0, and we can infer that adding more\n",
    "data will allow the estimator to very closely match the best\n",
    "possible cross-validation error.\n",
    "\n",
    "**When the learning curves have not yet converged with our full training set, it indicates an over-fit model.**\n",
    "\n",
    "An overfitting model can be improved by:\n",
    "\n",
    "- Gathering more training samples.\n",
    "- Using a less-sophisticated model (i.e. in this case, make ``d`` smaller)\n",
    "- Increasing regularization.\n",
    "\n",
    "In particular, gathering more features for each sample will not help the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Curves\n",
    "===========\n",
    "FIXME REWRITE THIS SECTION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify the effects of bias and variance and construct\n",
    "the best possible estimator, we will split our training data into\n",
    "a *training set* and a *validation set*.  As a general rule, the\n",
    "training set should be about 60% of the samples.\n",
    "\n",
    "The overarching idea is as follows. The model parameters (in our case,\n",
    "the coefficients of the polynomials) are learned using the training\n",
    "set as above. The error is evaluated on the validation set,\n",
    "and the meta-parameters (in our case, the degree of the polynomial)\n",
    "are adjusted so that this validation error is minimized.\n",
    "Finally, the labels are predicted for the test set. These labels\n",
    "are used to evaluate how well the algorithm can be expected to\n",
    "perform on unlabeled data.\n",
    "\n",
    "The validation error of our polynomial regression can be visualized\n",
    "by plotting the error as a function of the polynomial degree d. We can do\n",
    "this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# suppress warnings from Polyfit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Polyfit*')\n",
    "\n",
    "degrees = np.arange(21)\n",
    "train_err = np.zeros(len(degrees))\n",
    "validation_err = np.zeros(len(degrees))\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    p = np.polyfit(xtrain, ytrain, d)\n",
    "\n",
    "    train_err[i] = compute_error(xtrain, ytrain, p)\n",
    "    validation_err[i] = compute_error(xtest, ytest, p)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
    "ax.plot(degrees, train_err, lw=2, label = 'training error')\n",
    "ax.plot([0, 20], [error, error], '--k', label='intrinsic error')\n",
    "\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('degree of fit')\n",
    "ax.set_ylabel('rms error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure compactly shows the reason that validation is\n",
    "important. On the left side of the plot, we have very low-degree\n",
    "polynomial, which under-fits the data. This leads to a very high\n",
    "error for both the training set and the validation set. On\n",
    "the far right side of the plot, we have a very high degree\n",
    "polynomial, which over-fits the data. This can be seen in the fact\n",
    "that the training error is very low, while the validation\n",
    "error is very high. Plotted for comparison is the intrinsic error\n",
    "(this is the scatter artificially added to the data: click on the\n",
    "above image to see the source code). For this toy dataset,\n",
    "error = 1.0 is the best we can hope to attain. Choosing `d=6` in\n",
    "this case gets us very close to the optimal error.\n",
    "\n",
    "The astute reader will realize that something is amiss here: in\n",
    "the above plot, `d = 6` gives the best results. But in the previous\n",
    "plot, we found that `d = 6` vastly over-fits the data. What’s going\n",
    "on here? The difference is the **number of training points** used.\n",
    "In the previous example, there were only eight training points.\n",
    "In this example, we have 100. As a general rule of thumb, the more\n",
    "training points used, the more complicated model can be used.\n",
    "But how can you determine for a given model whether more training\n",
    "points will be helpful? A useful diagnostic for this are learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve seen above that an under-performing algorithm can be due\n",
    "to two possible situations: under-fitting and over-fitting. In order to evaluate our algorithm, we\n",
    "set aside a portion of our training data for cross-validation.\n",
    "Using the technique of learning curves, we can train on progressively\n",
    "larger subsets of the data, evaluating the training error and\n",
    "cross-validation error to determine whether our algorithm is overfitting or underfitting. But what do we do with this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "If our algorithm is **underfitting**, the following actions might help:\n",
    "\n",
    "- **Add more features**. In our example of predicting home prices,\n",
    "  it may be helpful to make use of information such as the neighborhood\n",
    "  the house is in, the year the house was built, the size of the lot, etc.\n",
    "  Adding these features to the training and test sets can improve\n",
    "  the fit.\n",
    "- **Use a more sophisticated model**. Adding complexity to the model can\n",
    "  help improve the fit. For a polynomial fit, this can be accomplished\n",
    "  by increasing the degree d. Each learning technique has its own\n",
    "  methods of adding complexity.\n",
    "- **Use fewer samples**. Though this will not improve the classification,\n",
    "  an underfitting algorithm can attain nearly the same error with a smaller\n",
    "  training sample. For algorithms which are computationally expensive,\n",
    "  reducing the training sample size can lead to very large improvements\n",
    "  in speed.\n",
    "- **Decrease regularization**. Regularization is a technique used to impose\n",
    "  simplicity in some machine learning models, by adding a penalty term that\n",
    "  depends on the characteristics of the parameters. If a model is underfitting,\n",
    "  decreasing the regularization can lead to better results.\n",
    "  \n",
    "### Overfitting\n",
    "\n",
    "If our algorithm shows signs of **overfitting**, the following actions might help:\n",
    "\n",
    "- **Use fewer features**. Using a feature selection technique may be\n",
    "  useful, and decrease the over-fitting of the estimator.\n",
    "- **Use a simpler model**.  Model complexity and over-fitting go hand-in-hand.\n",
    "- **Use more training samples**. Adding training samples can reduce\n",
    "  the effect of over-fitting.\n",
    "- **Increase Regularization**. Regularization is designed to prevent\n",
    "  over-fitting. So increasing regularization\n",
    "  can lead to better results for overfitting models.\n",
    "\n",
    "These choices become very important in real-world situations. For example,\n",
    "due to limited telescope time, astronomers must seek a balance between\n",
    "observing a large number of objects, and observing a large number of\n",
    "features for each object. Determining which is more important for a\n",
    "particular learning task can inform the observing strategy that the\n",
    "astronomer employs. In a later exercise, we will explore the use of\n",
    "learning curves for the photometric redshift problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
