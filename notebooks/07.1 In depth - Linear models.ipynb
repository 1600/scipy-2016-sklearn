{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In depth with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are useful when little data is available or for very large feature spaces, as in text classification. In addition, they form a good case study for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All linear models for regression learn a coefficient parameter ``coef_`` and an offset ``intercept_`` to make predictions using a linear combination of features:\n",
    "```\n",
    "y_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the linear models for regression is what kind of restrictions are put on ``coef_`` and ``intercept_`` (know as regularization), in addition to fitting the training data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most standard linear model is the 'ordinary least squares regression', often simply called 'linear regression'. It doesn't put any additional restrictions on ``coef_``, so when the number of features is large, it becomes ill-posed and the model overfits.\n",
    "\n",
    "Let us generate a simple simulation, to see the behavior of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = rng.normal(size=(1000, 50))\n",
    "beta = rng.normal(size=50)\n",
    "y = np.dot(X, beta) + 4 * rng.normal(size=1000)\n",
    "\n",
    "from sklearn import linear_model, cross_validation\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(est, X, y)\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--', label=\"training scores \" + estimator_name)\n",
    "    plt.plot(training_set_size,test_scores.mean(axis=1), '-', label=\"test scores \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "plot_learning_curve(linear_model.LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ordinary linear regression is not defined if there are less training samples than features. In the presence of noise, it does poorly as long as the number of samples is not several times the number of features.\n",
    "\n",
    "The LinearRegression is then overfitting: fitting noise. We need to regularize.\n",
    "\n",
    "**The Ridge estimator** is a simple regularization (called l2 penalty) of the ordinary LinearRegression. In particular, it has the benefit of being not computationaly more expensive than the ordinary least square estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(linear_model.LinearRegression(), X, y)\n",
    "plot_learning_curve(linear_model.Ridge(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the low-sample limit, the Ridge estimator performs much better than the unregularized model.\n",
    "\n",
    "The regularization of the Ridge is a shrinkage: the coefficients learned are biased towards zero. Too much bias is not beneficial, but with very few samples, we will need more bias.\n",
    "\n",
    "The amount of regularization is set via the `alpha` parameter of the Ridge. Tuning it is critical for performance. We can set it automatically by cross-validation using the RidgeCV estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(linear_model.LinearRegression(), X, y)\n",
    "plot_learning_curve(linear_model.Ridge(), X, y)\n",
    "plot_learning_curve(linear_model.RidgeCV(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Lasso estimator** is useful to impose sparsity on the coefficient. In other words, it is to be prefered if we believe that many of the features are not relevant. This is done via the so-called l1 penalty.\n",
    "\n",
    "Let us create such a situation with a new simulation where only 10 out of the 50 features are relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta[10:] = 0\n",
    "y = np.dot(X, beta) + 4*rng.normal(size=1000)\n",
    "plot_learning_curve(linear_model.RidgeCV(), X, y)\n",
    "plot_learning_curve(linear_model.Lasso(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Lasso estimator performs, in our case, better than the Ridge when there are a small number of training observations. However when there are a lot of observations the Lasso under-performs. Indeed, the variance-reducing effect of the regularization is less critical in these situations, and the bias becomes too detrimental.\n",
    "\n",
    "As with any estimator, we should tune the regularization parameter to get the best prediction. For this purpose, we can use the LassoCV object. Note that it is a significantly more computationally costly operation than the RidgeCV. To speed it up, we reduce the number of values explored for the alpha parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(linear_model.RidgeCV(), X, y)\n",
    "plot_learning_curve(linear_model.LassoCV(n_alphas=20), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNet** sits in between Lasso and Ridge. It has a tuning parameter, l1_ratio, that controls this behavior: when set to 0 (only l2 penalty), ElasticNet is a Ridge, when set to 1 (only l1 penalty), it is a Lasso. It is useful when your coefficients are not that sparse. The sparser the coefficients, the higher we should set l1_ratio. Note that l1_ratio can also be set by cross-validation, although we won't do it here to limit computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(linear_model.RidgeCV(), X, y)\n",
    "plot_learning_curve(linear_model.ElasticNetCV(l1_ratio=.6, n_alphas=20), X, y)\n",
    "plot_learning_curve(linear_model.LassoCV(n_alphas=20), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical tips:**\n",
    "\n",
    "- *Different algorithms for the same problem:* The Lasso model can be solved with different algorithms: the **Lasso** estimator implements a coordinate-descent approach; the **LassoLars** estimator implements the LARS alorithm. Depending on the situation, one may be faster than the other.\n",
    "\n",
    "- *Normalization:* as with all estimators, it can be useful to normalize your data. The linear models have a 'normalize' option that does the normalization, runs the models, and scales the coefficients so that they are suitable for non-normalized data. Note that at the time of writing Lasso and LassoLars have different default settings for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find the best linear regression prediction on the `diabetes` dataset, that is available in the scikit-learn datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All linear models for classification learn a coefficient parameter ``coef_`` and an offset ``intercept_`` to make predictions using a linear combination of features:\n",
    "```\n",
    "y_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_ > 0\n",
    "```\n",
    "As you can see, this is very similar to regression, only that a threshold at zero is applied.\n",
    "\n",
    "Again, the difference between the linear models for classification what kind of regularization is put on ``coef_`` and ``intercept_``, but there are also minor differences in how the fit to the training set is measured (the so-called loss function).\n",
    "\n",
    "The two most common models for linear classification are the linear SVM as implemented in LinearSVC and LogisticRegression.\n",
    "Both support ``l1`` and ``l2`` penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2-class classification task can be seen as a regression problem with output variables (y) drawn from [0, 1]. Let us simulate a simple example and applied linear regression to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=100)\n",
    "y = (X > 0).astype(np.float)\n",
    "X[X > 0] *= 4\n",
    "X += .3 * np.random.normal(size=100)\n",
    "X = X[:, np.newaxis]\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "# Fit a linear regression to it\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X, y)\n",
    "X_test = np.linspace(-4, 10, 100)[:, np.newaxis]\n",
    "plt.plot(X_test, ols.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a linear fit is clearly not a good fit for variable in [0, 1].\n",
    "\n",
    "A better approach is to fit a sigmoid, this is called the **logistic regression**. The sigmoid can be seen as coming from a probabilistic model of 'y'.\n",
    "\n",
    "In scikit-learn, some estimators have a 'predict_proba' method that gives the probablity of each class under the model learned. Let us use the predict_proba method of the LogisticRegression object to plot the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First plot the data and the ordinary least square regression\n",
    "plt.plot(X, y, 'o')\n",
    "plt.plot(X_test, ols.predict(X_test))\n",
    "\n",
    "# Plot a logistic regression\n",
    "log_reg = linear_model.LogisticRegression(C=1e5)\n",
    "log_reg.fit(X, y)\n",
    "plt.plot(X_test, log_reg.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization**: the logistic regression suffers from over-fit in the presence of many features and must be regularized. The 'C' parameter controls that regularization: large C values give unregularized model, while small C give strongly regularized models.\n",
    "\n",
    "A good intuition for regularization of linear classifiers is that with high regularization, it is enough if most of the points are classified correctly. But with less regularization, more importance is given to each individual data point.\n",
    "This is illustrated using an linear SVM with different values of ``C`` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from figures import plot_linear_svc_regularization\n",
    "plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar to the Ridge/Lasso separation, you can set the 'penalty' parameter to 'l1' to enforce sparsity of the coefficients.\n",
    "\n",
    "**Multi-class**: Multi-class with the LogisticRegression object is tackled using a one-vs-all approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use LogisticRegression to classify digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
