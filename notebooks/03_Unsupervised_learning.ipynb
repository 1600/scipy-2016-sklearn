{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "**Unsupervised Learning** addresses a different sort of problem. Here the data has no labels,\n",
    "and we are interested in finding similarities between the objects in question. In a sense,\n",
    "you can think of unsupervised learning as a means of discovering labels from the data itself.\n",
    "Unsupervised learning comprises tasks such as *dimensionality reduction*, *clustering*, and\n",
    "*density estimation*. For example, in the iris data discussed above, we can used unsupervised\n",
    "methods to determine combinations of the measurements which best display the structure of the\n",
    "data. As we’ll see below, such a projection of the data can be used to visualize the\n",
    "four-dimensional dataset in two dimensions. Some more involved unsupervised learning problems are:\n",
    "\n",
    "- given detailed observations of distant galaxies, determine which features or combinations of\n",
    "  features summarize best the information.\n",
    "- given a mixture of two sound sources (for example, a person talking over some music),\n",
    "  separate the two (this is called the [blind source separation](http://en.wikipedia.org/wiki/Blind_signal_separation) problem).\n",
    "- given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
    "\n",
    "Sometimes the two may even be combined: e.g. Unsupervised learning can be used to find useful\n",
    "features in heterogeneous data, and then these features can be used within a supervised\n",
    "framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example PCA for visualization** Principle Component Analysis (PCA) is a dimension reduction technique that can find the combinations of variables that explain the most variance.\n",
    "\n",
    "Consider the iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print \"Reduced dataset shape:\", X_reduced.shape\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
    "\n",
    "print \"Meaning of the 2 components:\"\n",
    "for component in pca.components_:\n",
    "    print \" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component, iris.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**: Clustering groups together observations that are homogeneous with respect to a given criterion, finding ''clusters'' in the data.\n",
    "\n",
    "Note that these clusters will uncover relevent hidden structure of the data only if the criterion used highlights it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0) # Fixing the RNG in kmeans\n",
    "k_means.fit(X_reduced)\n",
    "y_pred = k_means.predict(X_reduced)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A recap on Scikit-learn's estimator interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn strives to have a uniform interface across all methods,\n",
    "and we’ll see examples of these below. Given a scikit-learn *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO MOVE WAY BACK\n",
    "\n",
    "### Regularization: what it is and why it is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train errors** Suppose you are using a 1-nearest neighbor estimator. How many errors do you expect on your train set?\n",
    "\n",
    "This tells us that:\n",
    "- Train set error is not a good measurement of prediction performance. You need to leave out a test set.\n",
    "- In general, we should accept errors on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example of regularization** The core idea behind regularization is that we are going to prefer models that are simpler, for a certain definition of ''simpler'', even if they lead to more errors on the train set.\n",
    "\n",
    "As an example, let's generate with a 9th order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "x = 2 * rng.rand(100) - 1\n",
    "\n",
    "f = lambda t: 1.2 * t ** 2 + .1 * t ** 3 - .4 * t ** 5 - .5 * t ** 9\n",
    "y = f(x) + .4 * rng.normal(size=100)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's fit a 4th order and a 9th order polynomial to the data. For this we need to engineer features: the n_th powers of x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 1, 100)\n",
    "\n",
    "plt.scatter(x, y, s=20)\n",
    "\n",
    "X = np.array([x**i for i in range(5)]).T\n",
    "X_test = np.array([x_test**i for i in range(5)]).T\n",
    "order4 = LinearRegression()\n",
    "order4.fit(X, y)\n",
    "plt.plot(x_test, order4.predict(X_test), label='4th order')\n",
    "\n",
    "X = np.array([x**i for i in range(10)]).T\n",
    "X_test = np.array([x_test**i for i in range(10)]).T\n",
    "order9 = LinearRegression()\n",
    "order9.fit(X, y)\n",
    "plt.plot(x_test, order9.predict(X_test), label='9th order')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.axis('tight')\n",
    "plt.title('Fitting a 4th and a 9th order polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your naked eyes, which model do you prefer, the 4th order one, or the 9th order one?\n",
    "\n",
    "Let's look at the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=20)\n",
    "plt.plot(x_test, f(x_test), label=\"truth\")\n",
    "plt.axis('tight')\n",
    "plt.title('Ground truth (9th order polynomial)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is ubiquitous in machine learning. Most scikit-learn estimators have a parameter to tune the amount of regularization. For instance, with k-NN, it is 'k', the number of nearest neighbors used to make the decision. k=1 amounts to no regularization: 0 error on the training set, whereas large k will push toward smoother decision boundaries in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Interactive Demo on linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the **svm_gui.py** file in the repository: https://github.com/jakevdp/sklearn_scipy2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow chart: how do I choose what to do with my data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://scikit-learn.org/dev/tutorial/machine_learning_map/index.html\"><img src=\"http://scikit-learn.org/dev/_static/ml_map.png\" width=100%></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
