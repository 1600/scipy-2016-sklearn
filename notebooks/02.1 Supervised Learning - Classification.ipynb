{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data on two-dimensional screens.\n",
      "\n",
      "We will illustrate some very simple examples before we move on to more \"real world\" data sets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classification\n",
      "========\n",
      "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_blobs\n",
      "X, y = make_blobs(centers=2, random_state=0)\n",
      "print(X.shape)\n",
      "print(y.shape)\n",
      "print(X[:5, :])\n",
      "print(y[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the data is two-dimensional, we can plot each sample as a point in two-dimensional space, with the first feature being the x-axis and the second feature being the y-axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
      "plt.xlabel(\"first feature\")\n",
      "plt.ylabel(\"second feature\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As classification is a supervised task, and we are interested in how well the model generalizes, we split our data into a training set,\n",
      "to built the model from, and a test-set, to evaluate how well our model performs on new data. The ``train_test_split`` function form the ``cross_validation`` module does that for us, by randomly splitting of 25% of the data for testing.\n",
      "![train test split](figures/train_test_split.svg)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The scikit-learn estimator API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. For instance a logistic regression is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All models in scikit-learn have a very consistent interface.\n",
      "First, we instantiate the estimator object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To built the model from our data, that is to learn how to classify new points, we call the ``fit`` function with the training data, and the corresponding training labels (the desired output for the training data point):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction = classifier.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can compare these against the true labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(prediction)\n",
      "print(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can evaluate our classifier quantiatively by measuring what fraction of predictions is correct. This is called **accuracy**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(prediction == y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is also a convenience function , ``score``, that all scikit-learn classifiers have to compute this directly from the test data:\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.score(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LogisticRegression is a so-called linear model,\n",
      "that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from figures import plot_2d_separator\n",
      "\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
      "plt.xlabel(\"first feature\")\n",
      "plt.ylabel(\"second feature\")\n",
      "plot_2d_separator(classifier, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Estimated parameters**: All the estimated parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(classifier.coef_)\n",
      "print(classifier.intercept_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another classifier: K Nearest Neighbors\n",
      "------------------------------------------------\n",
      "Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
      "\n",
      "The interface is exactly the same as for ``LogisticRegression above``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "knn = KNeighborsClassifier(n_neighbors=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We fit the model with out training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "knn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
      "plt.xlabel(\"first feature\")\n",
      "plt.ylabel(\"second feature\")\n",
      "plot_2d_separator(knn, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise\n",
      "=========\n",
      "Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}