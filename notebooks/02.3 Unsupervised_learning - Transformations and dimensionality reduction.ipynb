{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Unsupervised Learning\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many instances of unsupervised learning, such as dimensionality reduction, manifold learning and feature extraction, find a new representation of the input data without any additional input.\n",
      "The most simple example of this, which can barely be called learning, is rescaling the data to have zero mean and unit variance. This is a helpful preprocessing step for many machine learning models.\n",
      "\n",
      "Applying such a preprocessing has a very similar interface to the supervised learning algorithms we saw so far.\n",
      "Let's load the iris dataset and rescale it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = iris.data, iris.target\n",
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The iris dataset is not \"centered\" that is it has non-zero mean and the standard deviation is different for each component:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"mean : %s \" % X.mean(axis=0))\n",
      "print(\"standard deviation : %s \" % X.std(axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use a preprocessing method, we first import the estimator, here StandardScaler and instantiate it:\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with the classification and regression algorithms, we call ``fit`` to learn the model from the data. As this is an unsupervised model, we only pass ``X``, not ``y``. This simply estimates mean and standard deviation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can rescale our data by applying the ``transform`` (not ``predict``) method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "X_scaled = scaler.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``X_scaled`` has the same number of samples and features, but the mean was subtracted and all features were scaled to have unit standard deviation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X_scaled.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"mean : %s \" % X_scaled.mean(axis=0))\n",
      "print(\"standard deviation : %s \" % X_scaled.std(axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Principal Component Analysis\n",
      "============================"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An unsupervised transformation that is somewhat more interesting is Principle Component Analysis (PCA).\n",
      "It is a technique to reduce the dimensionality of the data, by creating a linear projection.\n",
      "That is, we find new features to represent the data that are a linear combination of the old data (i.e. we rotate it).\n",
      "\n",
      "The way PCA finds these new directions is by looking for the directions of maximum variance.\n",
      "Usually only few components that explain most of the variance in the data are kept. To illustrate how a rotation might look like, we first show it on two dimensional data and keep both principal components.\n",
      "\n",
      "We create a Gaussian blob that is rotated:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rnd = np.random.RandomState(42)\n",
      "X_blob = np.dot(rnd.normal(size=(100, 2)), rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n",
      "plt.scatter(X_blob[:, 0], X_blob[:, 1])\n",
      "plt.xlabel(\"feature 1\")\n",
      "plt.ylabel(\"feature 2\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As always, we instantiate our PCA model. By default all directions are kept."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we fit the PCA model with our data. As PCA is an unsupervised algorithm, there is no output ``y``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.fit(X_blob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can transform the data, projected on the principal components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = pca.transform(X_blob)\n",
      "\n",
      "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
      "plt.xlabel(\"first principal component\")\n",
      "plt.ylabel(\"second principal component\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On the left of the plot you can see the four points that were on the top right before. PCA found fit first component to be along the diagonal, and the second to be perpendicular to it. As PCA finds a rotation, the principal components are always at right angles to each other."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dimensionality Reduction for Visualization with PCA\n",
      "-------------------------------------------------------------\n",
      "Consider the iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we load the iris dataset again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = iris.data, iris.target\n",
      "print(X.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This time, we set the number of components to ``2``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we project the data down to the first two principal components. The data has as many samples as before (as many flowers), but each is now represented by only two float numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_reduced = pca.transform(X)\n",
      "print(\"Reduced dataset shape: %s\" % (X_reduced.shape, ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now easily visualize the data in 2D:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
      "\n",
      "print(\"Meaning of the 2 components:\")\n",
      "for component in pca.components_:\n",
      "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
      "                     for value, name in zip(component, iris.feature_names)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#FIXME\n",
      "\n",
      "Exercise?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}